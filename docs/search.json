[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GSOC 2025 - Pitch Correction for Sound Playback in Sequencer Blog",
    "section": "",
    "text": "Exploring the Unknown Audaspace\n\n\n\nc++\n\naudaspace\n\nblender\n\n\n\nA collection of random notes for the week of 2025-06-09\n\n\n\n\n\nJun 14, 2025\n\n\nTheKaceFiles\n\n\n\n\n\n\n\n\n\n\n\n\nOutput WAV Files\n\n\n\nc++\n\naudaspace\n\nblender\n\n\n\nWav File Examples\n\n\n\n\n\nJun 14, 2025\n\n\nTheKaceFiles\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding VSE Audio in the Sequencer\n\n\n\nblender\n\nc++\n\n\n\nGeneral notes on how audio and pitch works in Blender’s VSE\n\n\n\n\n\nJun 7, 2025\n\n\nTheKaceFiles\n\n\n\n\n\n\n\n\n\n\n\n\nJune 2th - Note Dump\n\n\n\nnote-dump\n\n\n\nA collection of random notes for 2025-06\n\n\n\n\n\nJun 2, 2025\n\n\nTheKaceFiles\n\n\n\n\n\n\n\n\n\n\n\n\nAdding External Libraries in Blender\n\n\n\nblender\n\nc++\n\n\n\nWIP: Integrating the Rubber Band Library in Blender\n\n\n\n\n\nMay 25, 2025\n\n\nTheKaceFiles\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Signal Notes\n\n\n\ndsp\n\npython\n\n\n\nA collection of Digital Signal Processing notes\n\n\n\n\n\nMay 24, 2025\n\n\nTheKaceFiles\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMay 23, 2025\n\n\nTheKaceFiles\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/signal_review/index.html",
    "href": "posts/signal_review/index.html",
    "title": "Random Signal Notes",
    "section": "",
    "text": "We’ll first import the following libraries:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio\n\n\nSuppose we have the following signal which is sampled at 50 Hz and has a duration of 2 seconds.\n\n\nCode\nfs = 50\nduration = 2\nfreq = 2\nN = np.arange(int(fs * duration))\nt = N / fs       \ny = np.cos(2 * np.pi * freq * t)\n\n\nplt.plot(t, y)\nplt.xlabel(\"Seconds\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"Wave Example\")\n\nplt.grid(True)\n\n\n\n\n\n\n\n\nFigure 1: Signal example\n\n\n\n\n\nThe signal above has a frequency of 2 Hz (fs) which means that it completes 2 cycles in 1 second.\n\n\n\n\n\n\nDefinition: Period\n\n\n\nThe period of the signal is how long it takes for a signal to complete 1 cycle by taking\n\\[\nt_0 = \\frac{1}{f_s}\n\\]\nwhere \\(f_s\\) is the frequency of the signal.\n\n\nIn Figure 1, the signal has a period of \\(t_0 = \\frac{1}{f_s} = \\frac{1}{2} = 0.5\\) seconds."
  },
  {
    "objectID": "posts/signal_review/index.html#signals",
    "href": "posts/signal_review/index.html#signals",
    "title": "Random Signal Notes",
    "section": "",
    "text": "We’ll first import the following libraries:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio\n\n\nSuppose we have the following signal which is sampled at 50 Hz and has a duration of 2 seconds.\n\n\nCode\nfs = 50\nduration = 2\nfreq = 2\nN = np.arange(int(fs * duration))\nt = N / fs       \ny = np.cos(2 * np.pi * freq * t)\n\n\nplt.plot(t, y)\nplt.xlabel(\"Seconds\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"Wave Example\")\n\nplt.grid(True)\n\n\n\n\n\n\n\n\nFigure 1: Signal example\n\n\n\n\n\nThe signal above has a frequency of 2 Hz (fs) which means that it completes 2 cycles in 1 second.\n\n\n\n\n\n\nDefinition: Period\n\n\n\nThe period of the signal is how long it takes for a signal to complete 1 cycle by taking\n\\[\nt_0 = \\frac{1}{f_s}\n\\]\nwhere \\(f_s\\) is the frequency of the signal.\n\n\nIn Figure 1, the signal has a period of \\(t_0 = \\frac{1}{f_s} = \\frac{1}{2} = 0.5\\) seconds."
  },
  {
    "objectID": "posts/signal_review/index.html#notes-on-music-notes",
    "href": "posts/signal_review/index.html#notes-on-music-notes",
    "title": "Random Signal Notes",
    "section": "Notes on Music Notes",
    "text": "Notes on Music Notes\nThe human ear can hear frequencies between 20 Hz and 20 kHz. The sampling rate is 44.1 kHZ because of something called the Nyquist–Shannon sampling theorem which states that the sampling rate has to be at least twice of the maximum frequency of the signal which indeed \\(2 \\cdot 20 \\text{ kHz} &lt; 44.1 \\text{ kHZ}\\). There’s also more history for why the sampling rate is 44.1 kHZ here.\nNow, as for the frequencies of musical notes in Western music, we can use the formula below:\n\\[\nf = 2^{(n/12)} * 440\n\\]\nThe formula represents the frequency of the music note that is \\(n\\) semitones away from A4 (440 Hz). Each increasing semitone step increases the frequency by the ratio of \\(2^{(n/12)}\\).\n\n\n\n\n\n\nExamples\n\n\n\n\\(n = 0 \\rightarrow 2^{(0/12)} * 440 = 440 \\text{ Hz} = A4\\)\n\\(n = 1 \\rightarrow 2^{(1/12)} * 440 \\approx 466.163 \\text{ Hz} = A \\sharp 4 / B\\flat 4\\)\n\\(n = 12 \\rightarrow 2^{(12/12)} * 440 = 880 \\text{ Hz} = A5\\)\n\\(n = -12 \\rightarrow 2^{(-12/12)} * 440 = 220 \\text{ Hz} = A3\\)\n\n\nNotice that doubling the frequency represents an octave increase of a music note. Now, if we assume that 20 kHz is the maximum frequency that the human ear can hear, then the highest named musical note can be found by solving for \\(n\\) when \\(f =  20 \\text{ kHz}\\).\n\\[\n\\begin{align*}\n20000 &= 2^{\\frac{n}{12}} \\cdot 440 \\\\\n\\frac{20000}{440} &= 2^{\\frac{n}{12}} \\\\\n45.45 &= 2^{\\frac{n}{12}} \\\\\n\\log_2(45.45) &= \\frac{n}{12} \\\\\nn &= 12 \\cdot \\log_2(45.45) \\\\\nn &\\approx 66.0745\n\\end{align*}\n\\]\nWhen \\(n \\approx 66\\), this is 66 semitones aboves A4 and corresponds to the note \\(D \\sharp 10/ E\\flat 10\\).\nLet’s take a listen of the music notes, starting from A3 to D#10!\n\n\nCode\nfs = 44100\nnote_duration = 0.5\npause_duration = 0.10\n\nN = np.arange(int(fs * note_duration))\nt = N / fs\n\npause_arr = np.zeros(int(fs * pause_duration))\n\ndef get_frequency(n):\n    \"\"\"\n    Return the frequency of the note n semitones starting from A4 (440 Hz).\n    \"\"\"\n    return 2**(n / 12) * 440\n\n\n\naudio_sequence = np.array([])\nfor n in range(-12, 67):\n  freq = get_frequency(n)\n  signal_arr = np.cos(2 * np.pi * freq * t)\n  audio_sequence = np.concatenate((audio_sequence, signal_arr, pause_arr))\n\nAudio(data=audio_sequence, rate=fs)\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/audaspace_test/index.html",
    "href": "posts/audaspace_test/index.html",
    "title": "Exploring the Unknown Audaspace",
    "section": "",
    "text": "Over the past week, I’ve been exploring Audaspace’s codebase and messing around with the library to get a better intuition into integrating The Rubber Band Library for pitch correction. In particular, I’m going to explore Audaspace’s Sequence and SequenceEntry class and utilizing the AnimateableProperty variable for adjusting pitch, Blender’s VSE utilizes these classes. I’ll be using the Ducktales Moon Theme (1989) by Tonomura Hiroshige and the Ducktales NES Theme (1989) composed by Mark Mueller as the test track(s) for experimentation.\nMoon Theme (dt_moon_theme.mp3)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMain Theme (dt_main_theme.mp3)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nIn particular, I am going to explore animating pitch and applying different effects from Audaspace’s fx folder."
  },
  {
    "objectID": "posts/audaspace_test/index.html#intro",
    "href": "posts/audaspace_test/index.html#intro",
    "title": "Exploring the Unknown Audaspace",
    "section": "",
    "text": "Over the past week, I’ve been exploring Audaspace’s codebase and messing around with the library to get a better intuition into integrating The Rubber Band Library for pitch correction. In particular, I’m going to explore Audaspace’s Sequence and SequenceEntry class and utilizing the AnimateableProperty variable for adjusting pitch, Blender’s VSE utilizes these classes. I’ll be using the Ducktales Moon Theme (1989) by Tonomura Hiroshige and the Ducktales NES Theme (1989) composed by Mark Mueller as the test track(s) for experimentation.\nMoon Theme (dt_moon_theme.mp3)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMain Theme (dt_main_theme.mp3)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nIn particular, I am going to explore animating pitch and applying different effects from Audaspace’s fx folder."
  },
  {
    "objectID": "posts/audaspace_test/index.html#loading-in-audio-files",
    "href": "posts/audaspace_test/index.html#loading-in-audio-files",
    "title": "Exploring the Unknown Audaspace",
    "section": "Loading in Audio Files",
    "text": "Loading in Audio Files\nAudaspace provides a way to load in audio files through its File class. The example program below simply reads the dt_moon_theme.mp3 and then writes to example1.wav using the FileWriter class after running ./example1 dt_moon_theme.mp3\n\n\nCode\n\n// example1.cpp\n#include &lt;iostream&gt;\n#include \"file/File.h\"\n#include \"file/FileWriter.h\"\nusing namespace aud;\n\nint main(int argc, char* argv[])\n{\n    if(argc != 2)\n    {\n        std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" &lt;filename&gt;\" &lt;&lt; std::endl;\n        return 1;\n    }\n  \n    File file(argv[1]);\n  \n    StreamInfo streamInfo = file.queryStreams()[0];\n\n    DeviceSpecs outspecs = streamInfo.specs;\n\n    std::shared_ptr&lt;IWriter&gt; writer = FileWriter::createWriter(\"example1.wav\", outspecs, CONTAINER_WAV, CODEC_PCM, 0);\n    \n    int totalsamples = int(outspecs.specs.rate * streamInfo.duration);\n    printf(\"Total Samples: %d\\n\", totalsamples);\n    FileWriter::writeReader(file.createReader(), writer, totalsamples, AUD_DEFAULT_BUFFER_SIZE);\n}"
  },
  {
    "objectID": "posts/audaspace_test/index.html#audaspaces-sequence-and-sequenceentry",
    "href": "posts/audaspace_test/index.html#audaspaces-sequence-and-sequenceentry",
    "title": "Exploring the Unknown Audaspace",
    "section": "Audaspace’s Sequence and SequenceEntry",
    "text": "Audaspace’s Sequence and SequenceEntry\nSuppose we want to combine dt_moon_theme.mp3 and dt_main_theme.mp3 (or as many we want) into one wav file with a 3 second interval between each audio clip. An approach to do would be using Audaspace’s Sequence class which allows sequenced entries of audio to be played. In Blender’s VSE, it would be represented like so:\n\nIt turns out, each audio strip contains a variable called scene_sound that is a pointer to AUD_SequenceEntry or SequenceEntry. And the Sequence class stores a list of SequenceEntrys. The code for outputting the combined audio file would look like the following:\n\n\nCode\n\n// example2.cpp\n#include &lt;iostream&gt;\n#include \"file/File.h\"\n#include \"file/FileWriter.h\"\n#include \"sequence/Sequence.h\"\n#include \"plugin/PluginManager.h\"\nusing namespace aud;\n\nint main(int argc, char* argv[])\n{\n\n  if (argc &lt; 2) {\n        std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" &lt;file1&gt; &lt;file2&gt; ...\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n  // For some reason, PluginManager::loadPlugins(\"\") needs to be called for this program to not error?\n  PluginManager::loadPlugins(\"\");\n  const double FPS = 30.0;\n  const double INTERVAL_BETWEEN_AUDIO = 3.0;\n \n  SampleRate sampleRate = RATE_44100;\n  Specs specs { sampleRate, CHANNELS_MONO };\n  std::shared_ptr&lt;Sequence&gt; seq = std::shared_ptr&lt;Sequence&gt;(new Sequence(specs, FPS, false));\n \n  double position = 0;\n \n  for (int i = 1; i &lt; argc; i++) {\n    std::shared_ptr&lt;File&gt; file = std::make_shared&lt;File&gt;(argv[i]);\n    double duration = file.get()-&gt;queryStreams()[0].duration;\n\n    seq-&gt;add(file, position, position + duration, 0);\n    position += duration + INTERVAL_BETWEEN_AUDIO;\n  }\n\n  DeviceSpecs outspecs;\n    outspecs.channels = CHANNELS_MONO;\n    outspecs.rate = sampleRate;\n    outspecs.format = FORMAT_FLOAT32;\n\n\n    std::shared_ptr&lt;IWriter&gt; writer = FileWriter::createWriter(\"example2.wav\", outspecs, CONTAINER_WAV, CODEC_PCM, 0);\n    \n    int totalsamples = int(outspecs.specs.rate * position);\n    printf(\"Total Samples: %d\\n\", totalsamples);\n    FileWriter::writeReader(seq-&gt;createReader(), writer, totalsamples, AUD_DEFAULT_BUFFER_SIZE);\n\n}\n\nIn the code above, seq-&gt;add(file, position, position + duration, 0) creates the SequenceEntry and adds to an internal list, as shown by the following code in Audaspace’s SequenceData.cpp\n\n\nCode\n\nstd::shared_ptr&lt;SequenceEntry&gt; SequenceData::add(std::shared_ptr&lt;ISound&gt; sound, std::shared_ptr&lt;SequenceData&gt; sequence_data, double begin, double end, double skip)\n{\n    std::lock_guard&lt;std::recursive_mutex&gt; lock(m_mutex);\n\n    std::shared_ptr&lt;SequenceEntry&gt; entry = std::shared_ptr&lt;SequenceEntry&gt;(new SequenceEntry(sound, begin, end, skip, sequence_data, m_id++));\n\n    m_entries.push_back(entry);\n    m_entry_status++;\n\n    return entry;\n}\n\nRunning ./example2 dt_moon_theme.mp3 dt_main_theme.mp3 results in the following audio clip:\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nNotice the 3 second silence between 0:49 and 0:52 and also at the end!"
  },
  {
    "objectID": "posts/audaspace_test/index.html#animating-pitch",
    "href": "posts/audaspace_test/index.html#animating-pitch",
    "title": "Exploring the Unknown Audaspace",
    "section": "Animating Pitch",
    "text": "Animating Pitch\nNow suppose we wanted to have the 1st audio clip played at 2x speed. It would be represented like so in Blender’s VSE:\n\nOne thing to note is that playback speed and pitch are directly related, where increasing the playback speed by 2x doubles the frequency of a signal and hence increases the pitch by an octave. We can replicate this effect by setting the animation property with the writeConstantRange just like how Blender’s VSE does.\n\n\nCode\n\n// example3.cpp\n#include &lt;iostream&gt;\n\n#include \"file/File.h\"\n#include \"file/FileWriter.h\"\n#include \"plugin/PluginManager.h\"\n#include \"sequence/Sequence.h\"\n#include \"sequence/SequenceEntry.h\"\nusing namespace aud;\n\nint main(int argc, char* argv[]) {\n  if (argc &lt; 2) {\n    std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" &lt;file1&gt; &lt;file2&gt; ...\" &lt;&lt; std::endl;\n    return 1;\n  }\n\n  // For some reason, PluginManager::loadPlugins(\"\") needs to be called for this program to not error?\n  PluginManager::loadPlugins(\"\");\n  const double FPS = 30.0;\n  const double INTERVAL_BETWEEN_AUDIO = 3.0;\n\n  SampleRate sampleRate = RATE_44100;\n  Specs specs{sampleRate, CHANNELS_MONO};\n  std::shared_ptr&lt;Sequence&gt; seq = std::shared_ptr&lt;Sequence&gt;(new Sequence(specs, FPS, false));\n\n  std::vector&lt;std::shared_ptr&lt;SequenceEntry&gt;&gt; entries;\n\n  double position = 0;\n\n  for (int i = 1; i &lt; argc; i++) {\n    std::shared_ptr&lt;File&gt; file = std::make_shared&lt;File&gt;(argv[i]);\n    double duration = file.get()-&gt;queryStreams()[0].duration;\n\n    std::shared_ptr&lt;SequenceEntry&gt; entry = seq-&gt;add(file, position, position + duration, 0);\n    entries.push_back(entry);\n    position += duration + INTERVAL_BETWEEN_AUDIO;\n  }\n\n  AnimateableProperty* prop = entries[0]-&gt;getAnimProperty(AP_PITCH);\n\n  float pitch = 2.0;\n\n  prop-&gt;writeConstantRange(&pitch, 0, 755);\n\n  DeviceSpecs outspecs;\n  outspecs.channels = CHANNELS_MONO;\n  outspecs.rate = sampleRate;\n  outspecs.format = FORMAT_FLOAT32;\n\n  std::shared_ptr&lt;IWriter&gt; writer = FileWriter::createWriter(\"example3.wav\", outspecs, CONTAINER_WAV, CODEC_PCM, 0);\n\n  int totalsamples = int(outspecs.specs.rate * position);\n  printf(\"Total Samples: %d\\n\", totalsamples);\n  FileWriter::writeReader(seq-&gt;createReader(), writer, totalsamples, AUD_DEFAULT_BUFFER_SIZE);\n}\n\nResults:\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\nAnimating Sound\nBlender uses the BKE_sound_set_scene_sound_volume_at_frame method which calls the Audaspace’s AUD_SequenceEntry_setAnimationData method which looks the following:\n\n\nCode\n\nAUD_API void AUD_SequenceEntry_setAnimationData(AUD_SequenceEntry* entry, AUD_AnimateablePropertyType type, int frame, float* data, char animated)\n{\n    AnimateableProperty* prop = (*entry)-&gt;getAnimProperty(static_cast&lt;AnimateablePropertyType&gt;(type));\n    if(animated)\n    {\n        if(frame &gt;= 0)\n            prop-&gt;write(data, frame, 1);\n    }\n    else\n    {\n        prop-&gt;write(data);\n    }\n}\n\nHow AUD_SequenceEntry_setAnimationData works is that if the animated flag is set, it sets the audio property at the particular frame. Otherwise, it sets the property for that entire entry.\nFor an animated example, take the following program\n\n\nCode\n\n// example4.cpp\n#include &lt;iostream&gt;\n\n#include \"file/File.h\"\n#include \"file/FileWriter.h\"\n#include \"plugin/PluginManager.h\"\n#include \"sequence/Sequence.h\"\n#include \"sequence/SequenceEntry.h\"\n\nusing namespace aud;\n\nint main(int argc, char* argv[]) {\n  if (argc &lt; 2) {\n    std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" &lt;file&gt;\" &lt;&lt; std::endl;\n    return 1;\n  }\n\n  // For some reason, PluginManager::loadPlugins(\"\") needs to be called for this program to not error?\n  PluginManager::loadPlugins(\"\");\n  const double FPS = 30.0;\n\n  SampleRate sampleRate = RATE_44100;\n  Specs specs{sampleRate, CHANNELS_MONO};\n  std::shared_ptr&lt;Sequence&gt; seq = std::shared_ptr&lt;Sequence&gt;(new Sequence(specs, FPS, false));\n  double position = 0;\n\n  std::shared_ptr&lt;File&gt; file = std::make_shared&lt;File&gt;(argv[1]);\n  double duration = file.get()-&gt;queryStreams()[0].duration;\n  std::shared_ptr&lt;SequenceEntry&gt; entry = seq-&gt;add(file, position, position + duration, 0);\n  position += duration;\n\n\n  AnimateableProperty* prop = entry-&gt;getAnimProperty(AP_VOLUME);\n\n  float volume = 0.2;\n  prop-&gt;write(&volume, FPS * 10, 1);\n\n  DeviceSpecs outspecs;\n  outspecs.channels = CHANNELS_MONO;\n  outspecs.rate = sampleRate;\n  outspecs.format = FORMAT_FLOAT32;\n\n  std::shared_ptr&lt;IWriter&gt; writer = FileWriter::createWriter(\"example4.wav\", outspecs, CONTAINER_WAV, CODEC_PCM, 0);\n\n  int totalsamples = int(outspecs.specs.rate * position);\n  printf(\"Total Samples: %d\\n\", totalsamples);\n  FileWriter::writeReader(seq-&gt;createReader(), writer, totalsamples, AUD_DEFAULT_BUFFER_SIZE);\n}\n\nIn this the program above, the volume is set to be 0.2 at the 300th frame (or 10 seconds into the audio) which sounds like so below:\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWe can also animate other values that we can animate too like pitch (AP_PITCH), panning (AP_PANNING), location (AP_LOCATION), and orientation (AP_ORIENTATION). Here’s what it sounds like when pitch (AP_PITCH) is animated instead from the program above (replace entry-&gt;getAnimProperty(AP_VOLUME) with entry-&gt;getAnimProperty(AP_PITCH)).\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nIt’s important to note that animated values like orientation take in array of values as demonstrated below:\nAnimateableProperty* prop = entry-&gt;getAnimProperty(AP_ORIENTATION);\nfloat m_v[4] = { 2, 2, 2, 2 };\nprop-&gt;write(m_v, FPS * 10, 1);"
  },
  {
    "objectID": "posts/audaspace_test/index.html#effects-and-effect-readers",
    "href": "posts/audaspace_test/index.html#effects-and-effect-readers",
    "title": "Exploring the Unknown Audaspace",
    "section": "Effects and Effect Readers",
    "text": "Effects and Effect Readers\nTo modify an existing sound, Audaspace uses something called an Effect and EffectReader for playback of the new audio. These effects can be composed/combined together to make new sounds. Audaspace implements a multitude of these effects in the fx folder. However, from what I understand, there’s currently only 1 effect used in Blender (the Equalizer, which anecdotally is a bit underdeveloped!). Some of the ones that are the most intuitive to understand is the Delay (which simply delays an audio by the specified amount in seconds), Loop (which repeats the audio by the specified amount), and Fader (which allows audio fade-in and audio fade-out).\nThere are more complex effects for people more familar with digital music production like IIRFilter and Envelope, and Convolver\nBelow are some example of the code using the simple effects:\n\n\nCode\n\n#include &lt;iostream&gt;\n#include \"file/File.h\"\n#include \"file/FileWriter.h\"\n#include \"plugin/PluginManager.h\"\n#include \"Audaspace.h\"\n#include \"IReader.h\"\n#include \"util/Buffer.h\"\n\n#include \"fx/Delay.h\"\n#include \"fx/Loop.h\"\n#include \"fx/Fader.h\"\n\nusing namespace aud;\n\nint main(int argc, char* argv[])\n{\n    if(argc != 2)\n    {\n        std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" &lt;filename&gt;\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n  PluginManager::loadPlugins(\"\");\n  \n    std::shared_ptr&lt;File&gt; file = std::make_shared&lt;File&gt;(argv[1]);\n  \n    StreamInfo streamInfo = file-&gt;queryStreams()[0];\n\n    DeviceSpecs outspecs = streamInfo.specs;\n\n\n\n  Delay delay = Delay(file, 3.0);\n  Loop loop = Loop(file, 1);\n  Fader fader = Fader(file, FADE_IN, 0, 20);\n\n  std::shared_ptr&lt;IWriter&gt; delayWriter = FileWriter::createWriter(\"delay.wav\", outspecs, CONTAINER_WAV, CODEC_PCM, 0);\n    std::shared_ptr&lt;IWriter&gt; loopWriter = FileWriter::createWriter(\"loop.wav\", outspecs, CONTAINER_WAV, CODEC_PCM, 0);\n  std::shared_ptr&lt;IWriter&gt; faderWriter = FileWriter::createWriter(\"fader.wav\", outspecs, CONTAINER_WAV, CODEC_PCM, 0);\n  \n    FileWriter::writeReader(delay.createReader(), delayWriter,  int(outspecs.specs.rate * (streamInfo.duration + 3)), AUD_DEFAULT_BUFFER_SIZE);\n  FileWriter::writeReader(loop.createReader(), loopWriter,  int(outspecs.specs.rate * (streamInfo.duration * 2)), AUD_DEFAULT_BUFFER_SIZE);\n  FileWriter::writeReader(fader.createReader(), faderWriter,  int(outspecs.specs.rate * streamInfo.duration), AUD_DEFAULT_BUFFER_SIZE);\n}\n\nYou can see output audio files here."
  },
  {
    "objectID": "posts/audaspace_test/index.html#implementing-timestretcher-and-timestretcherreader-with-rubberband",
    "href": "posts/audaspace_test/index.html#implementing-timestretcher-and-timestretcherreader-with-rubberband",
    "title": "Exploring the Unknown Audaspace",
    "section": "Implementing TimeStretcher and TimeStretcherReader with Rubberband",
    "text": "Implementing TimeStretcher and TimeStretcherReader with Rubberband\nUsing the Rubberband Library, I implemented a time stretcher effect here (note that the implementation probably doesn’t follow best practices). The most difficult part was moving between the interleaved format and deinterleaved format of audio, as the input audio is in interleaved format but the Rubberband Library expects the audio in an deinterleaved format, which then has to be converted the interleaved format after being time-stretched. If the audio was in Stereo for example, the interleaved format would look something like so: [ L1, R1, L2, R2, L3, R3, ... ]. Meanwhile, the deinterleaved format would look like:\nCH1: [ L1, L2, L3, ... ]\nCH2: [ R1, R2, R3, ... ]\nHere’s some example output audio using the TimeStretch effect at 2x and 0.5x speed here."
  },
  {
    "objectID": "posts/audaspace_test/index.html#questions",
    "href": "posts/audaspace_test/index.html#questions",
    "title": "Exploring the Unknown Audaspace",
    "section": "Questions",
    "text": "Questions\n\nWhen implementing time-stretching in Blender’s VSE, how should we handle audio scrubbing and changing the playback speed with the retiming keys while the audio is playing? Time-stretching is very expensive, and it would be good to see how other video editors overcome this…\nShould the real-time option of the Rubberband Library be used instead of the offline option. The difference between the options can be found here. Likely that real-time is used as the time ratio cannot be changed after the audio is “studied”? We can call the reset method on the stretcher and pass the original audio data again… (probably not a good idea)\nHandling speed transitions, so when audio is transitioning playback speed from 13% to 67%, how should that be handled? (Call the process method for the stretcher for the different speedw at each frame?)"
  },
  {
    "objectID": "posts/audaspace_test/index.html#pitch-correction-implemention-details-dump",
    "href": "posts/audaspace_test/index.html#pitch-correction-implemention-details-dump",
    "title": "Exploring the Unknown Audaspace",
    "section": "Pitch Correction Implemention Details Dump",
    "text": "Pitch Correction Implemention Details Dump\nThis may be wildly incorrect or off… but this so far how I imagine pitch correction can be implemented…\n\nEach SequenceEntry has an instance to some RubberbandStretcher object…\nReuse the pitch animated value to the set the time-stretching…\nIf pitch correction is enabled, the m_pitch variable should be set to 1 since the output buffer from the Rubberband is already pitch corrected. Replace the original buffer with the output buffer from Rubberband (check if this is possible…)\nReview implementation examples discussed here\nPerhaps use offline mode for final render of video? Use real-time mode for everything else?"
  },
  {
    "objectID": "posts/audaspace_rubberband/index.html",
    "href": "posts/audaspace_rubberband/index.html",
    "title": "Output WAV Files",
    "section": "",
    "text": "0.5x Speed\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n2x Speed\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\nDelay.wav\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\nLoop.wav\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\nFader.wav\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  }
]